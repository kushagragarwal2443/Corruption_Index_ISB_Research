{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v')) # past tense to present tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['disk', 'fail', 'time', 'like', 'replac']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview a document after preprocessing\n",
    "'''\n",
    "document_num = 50\n",
    "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./y1819.csv', error_bad_lines=False)\n",
    "datacolumn = \"Text\"\n",
    "# print(data)\n",
    "\n",
    "data_text = data[[datacolumn]]\n",
    "# print(data_text)\n",
    "\n",
    "data_text = data_text.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text\n",
      "0        The Indian economy is witnessing a “cyclical...\n",
      "1        Citizens’ date with democracy falls on a wee...\n",
      "2        India on April 24 said protectionism in all ...\n",
      "3        The government on Tuesday opened for public ...\n",
      "4       The price of petrol rose to a four-year high ...\n",
      "...                                                  ...\n",
      "59141   As an unusually hot summer takes off, outer z...\n",
      "59142   Japanese electronics company Murata Manufactu...\n",
      "59143   Ram Janmabhoomi is an emotional issue and can...\n",
      "59144   The Vidhana Soudha police arrested six people...\n",
      "59145   Senior Indian Police Service officer Param Bi...\n",
      "\n",
      "[59146 rows x 1 columns]\n",
      "59146\n"
     ]
    }
   ],
   "source": [
    "print(data_text)\n",
    "processed_docs=[]\n",
    "for datas in data_text[\"Text\"]:\n",
    "    processed_docs.append(preprocess(datas))\n",
    "    \n",
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['indian',\n",
       "  'economi',\n",
       "  'wit',\n",
       "  'cyclic',\n",
       "  'upsw',\n",
       "  'countri',\n",
       "  'like',\n",
       "  'clock',\n",
       "  'growth',\n",
       "  'financi',\n",
       "  'year',\n",
       "  'say',\n",
       "  'deutsch',\n",
       "  'bank',\n",
       "  'research',\n",
       "  'report',\n",
       "  'current',\n",
       "  'growth',\n",
       "  'forecast',\n",
       "  'estim',\n",
       "  'mark',\n",
       "  'improv',\n",
       "  'like',\n",
       "  'turn',\n",
       "  'global',\n",
       "  'financi',\n",
       "  'servic',\n",
       "  'major',\n",
       "  'say',\n",
       "  'reserv',\n",
       "  'bank',\n",
       "  'expect',\n",
       "  'india',\n",
       "  'econom',\n",
       "  'growth',\n",
       "  'rate',\n",
       "  'strengthen',\n",
       "  'current',\n",
       "  'fiscal',\n",
       "  'account',\n",
       "  'reviv',\n",
       "  'invest',\n",
       "  'activ',\n",
       "  'report',\n",
       "  'note',\n",
       "  'higher',\n",
       "  'global',\n",
       "  'price',\n",
       "  'risk',\n",
       "  'earlier',\n",
       "  'anticip',\n",
       "  'rate',\n",
       "  'hike',\n",
       "  'cycl',\n",
       "  'potenti',\n",
       "  'negat',\n",
       "  'impact',\n",
       "  'bank',\n",
       "  'sector',\n",
       "  'fraud',\n",
       "  'credit',\n",
       "  'overal',\n",
       "  'growth',\n",
       "  'factor',\n",
       "  'pose',\n",
       "  'downsid',\n",
       "  'risk',\n",
       "  'baselin',\n",
       "  'estim',\n",
       "  'brent',\n",
       "  'crude',\n",
       "  'price',\n",
       "  'current',\n",
       "  'hover',\n",
       "  'barrel',\n",
       "  'decemb',\n",
       "  'level',\n",
       "  'accord',\n",
       "  'deutsch',\n",
       "  'bank',\n",
       "  'research',\n",
       "  'report',\n",
       "  'increas',\n",
       "  'price',\n",
       "  'shave',\n",
       "  'growth',\n",
       "  'factor',\n",
       "  'pose',\n",
       "  'addit',\n",
       "  'downsid',\n",
       "  'risk',\n",
       "  'notwithstand',\n",
       "  'fact',\n",
       "  'higher',\n",
       "  'price',\n",
       "  'potenti',\n",
       "  'slow',\n",
       "  'pace',\n",
       "  'recoveri',\n",
       "  'econom',\n",
       "  'momentum',\n",
       "  'continu',\n",
       "  'improv',\n",
       "  'sequenti',\n",
       "  'report',\n",
       "  'note',\n",
       "  'capac',\n",
       "  'utilis',\n",
       "  'start',\n",
       "  'improv',\n",
       "  'incentivis',\n",
       "  'privat',\n",
       "  'sector',\n",
       "  'capex',\n",
       "  'recoveri',\n",
       "  'collect',\n",
       "  'pick',\n",
       "  'thank',\n",
       "  'implement',\n",
       "  'resolut',\n",
       "  'underway',\n",
       "  'govern',\n",
       "  'like',\n",
       "  'remain',\n",
       "  'focus',\n",
       "  'push',\n",
       "  'infrastructur',\n",
       "  'invest',\n",
       "  'bode',\n",
       "  'growth',\n",
       "  'outlook',\n",
       "  'go',\n",
       "  'forward',\n",
       "  'report',\n",
       "  'add'],\n",
       " ['citizen',\n",
       "  'date',\n",
       "  'democraci',\n",
       "  'fall',\n",
       "  'weekend',\n",
       "  'saturday',\n",
       "  'give',\n",
       "  'rise',\n",
       "  'fear',\n",
       "  'bengaluru',\n",
       "  'poor',\n",
       "  'scoreboard',\n",
       "  'voter',\n",
       "  'turnout',\n",
       "  'improv',\n",
       "  'assembl',\n",
       "  'elect',\n",
       "  'travel',\n",
       "  'agent',\n",
       "  'good',\n",
       "  'news',\n",
       "  'number',\n",
       "  'book',\n",
       "  'elect',\n",
       "  'weekend',\n",
       "  'fewer',\n",
       "  'usual',\n",
       "  'time',\n",
       "  'year',\n",
       "  'bengaluru',\n",
       "  'voter',\n",
       "  'percentag',\n",
       "  'stand',\n",
       "  'sunday',\n",
       "  'littl',\n",
       "  'higher',\n",
       "  'vote',\n",
       "  'cast',\n",
       "  'section',\n",
       "  'tour',\n",
       "  'oper',\n",
       "  'travel',\n",
       "  'agent',\n",
       "  'believ',\n",
       "  'thing',\n",
       "  'differ',\n",
       "  'elect',\n",
       "  'spokesperson',\n",
       "  'know',\n",
       "  'tour',\n",
       "  'oper',\n",
       "  'say',\n",
       "  'general',\n",
       "  'observ',\n",
       "  'peopl',\n",
       "  'vote',\n",
       "  'avoid',\n",
       "  'travel',\n",
       "  'date',\n",
       "  'prefer',\n",
       "  'postpon',\n",
       "  'trip',\n",
       "  'elect',\n",
       "  'sandeep',\n",
       "  'thamankar',\n",
       "  'run',\n",
       "  'travel',\n",
       "  'agenc',\n",
       "  'bengaluru',\n",
       "  'say',\n",
       "  'local',\n",
       "  'resid',\n",
       "  'enquir',\n",
       "  'possibl',\n",
       "  'voter',\n",
       "  'awar',\n",
       "  'social',\n",
       "  'media',\n",
       "  'know',\n",
       "  'peopl',\n",
       "  'minut',\n",
       "  'gurumurthi',\n",
       "  'owner',\n",
       "  'travel',\n",
       "  'agenc',\n",
       "  'say',\n",
       "  'half',\n",
       "  'number',\n",
       "  'book',\n",
       "  'usual',\n",
       "  'summer',\n",
       "  'vacat',\n",
       "  'time',\n",
       "  'lesser',\n",
       "  'book',\n",
       "  'usual',\n",
       "  'peak',\n",
       "  'season',\n",
       "  'april',\n",
       "  'depend',\n",
       "  'exam',\n",
       "  'result',\n",
       "  'school',\n",
       "  'reopen',\n",
       "  'packag',\n",
       "  'tour',\n",
       "  'batch',\n",
       "  'time',\n",
       "  'arrang',\n",
       "  'reason',\n",
       "  'teacher',\n",
       "  'govern',\n",
       "  'offici',\n",
       "  'bind',\n",
       "  'poll',\n",
       "  'duti',\n",
       "  'travel',\n",
       "  'say',\n",
       "  'add',\n",
       "  'travel',\n",
       "  'destin',\n",
       "  'north',\n",
       "  'india',\n",
       "  'shimla',\n",
       "  'manali',\n",
       "  'say',\n",
       "  'wit',\n",
       "  'demand',\n",
       "  'holiday',\n",
       "  'packag',\n",
       "  'employe',\n",
       "  'travel',\n",
       "  'agenc',\n",
       "  'say',\n",
       "  'book',\n",
       "  'weekend',\n",
       "  'higher',\n",
       "  'usual',\n",
       "  'peopl',\n",
       "  'book',\n",
       "  'packag',\n",
       "  'shimla',\n",
       "  'manali',\n",
       "  'mysuru',\n",
       "  'kodagu',\n",
       "  'weekend',\n",
       "  'summer',\n",
       "  'vacat',\n",
       "  'peopl',\n",
       "  'usual',\n",
       "  'travel',\n",
       "  'cooler',\n",
       "  'destin',\n",
       "  'happen',\n",
       "  'year',\n",
       "  'say']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accord\n",
      "1 account\n",
      "2 activ\n",
      "3 add\n",
      "4 addit\n",
      "5 anticip\n",
      "6 bank\n",
      "7 barrel\n",
      "8 baselin\n",
      "9 bode\n",
      "10 brent\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 29 (\"forward\") appears 2 time.\n",
      "Word 58 (\"rate\") appears 1 time.\n",
      "Word 59 (\"recoveri\") appears 3 time.\n",
      "Word 72 (\"turn\") appears 1 time.\n",
      "Word 127 (\"reason\") appears 1 time.\n",
      "Word 155 (\"agricultur\") appears 1 time.\n",
      "Word 172 (\"commit\") appears 1 time.\n",
      "Word 284 (\"regim\") appears 1 time.\n",
      "Word 330 (\"maharashtra\") appears 1 time.\n",
      "Word 340 (\"pump\") appears 4 time.\n",
      "Word 438 (\"loan\") appears 1 time.\n",
      "Word 513 (\"electr\") appears 3 time.\n",
      "Word 604 (\"step\") appears 1 time.\n",
      "Word 606 (\"suppli\") appears 2 time.\n",
      "Word 647 (\"hour\") appears 4 time.\n",
      "Word 694 (\"capabl\") appears 1 time.\n",
      "Word 795 (\"despit\") appears 1 time.\n",
      "Word 931 (\"onlin\") appears 1 time.\n",
      "Word 1102 (\"appeal\") appears 3 time.\n",
      "Word 1103 (\"attempt\") appears 1 time.\n",
      "Word 1104 (\"authoritarian\") appears 1 time.\n",
      "Word 1105 (\"bill\") appears 2 time.\n",
      "Word 1106 (\"chandrashekhar\") appears 1 time.\n",
      "Word 1107 (\"contempt\") appears 1 time.\n",
      "Word 1108 (\"cut\") appears 1 time.\n",
      "Word 1109 (\"drastic\") appears 1 time.\n",
      "Word 1110 (\"due\") appears 5 time.\n",
      "Word 1111 (\"farmer\") appears 8 time.\n",
      "Word 1112 (\"kalida\") appears 1 time.\n",
      "Word 1113 (\"petit\") appears 1 time.\n",
      "Word 1114 (\"realiti\") appears 1 time.\n",
      "Word 1115 (\"rupe\") appears 1 time.\n",
      "Word 1116 (\"scheme\") appears 1 time.\n",
      "Word 1117 (\"shetkari\") appears 1 time.\n",
      "Word 1118 (\"singl\") appears 1 time.\n",
      "Word 1119 (\"stay\") appears 2 time.\n",
      "Word 1120 (\"subsidis\") appears 3 time.\n",
      "Word 1121 (\"suppos\") appears 1 time.\n",
      "Word 1122 (\"verdict\") appears 2 time.\n",
      "Word 1123 (\"waiv\") appears 2 time.\n",
      "Word 1124 (\"waiver\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 25, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 8,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.041*\"tamil\" + 0.036*\"nadu\" + 0.018*\"aiadmk\" + 0.013*\"pakistan\" + 0.012*\"chennai\" + 0.008*\"stalin\" + 0.007*\"prime\" + 0.007*\"talk\" + 0.007*\"palaniswami\" + 0.006*\"modi\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.065*\"farmer\" + 0.026*\"price\" + 0.022*\"agricultur\" + 0.018*\"crop\" + 0.017*\"farm\" + 0.011*\"land\" + 0.010*\"suppli\" + 0.009*\"market\" + 0.009*\"product\" + 0.009*\"cultiv\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.033*\"seat\" + 0.030*\"poll\" + 0.030*\"candid\" + 0.028*\"constitu\" + 0.027*\"sabha\" + 0.021*\"vote\" + 0.019*\"contest\" + 0.018*\"voter\" + 0.016*\"allianc\" + 0.011*\"elector\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.030*\"vehicl\" + 0.021*\"transport\" + 0.020*\"airport\" + 0.015*\"traffic\" + 0.013*\"passeng\" + 0.012*\"bus\" + 0.011*\"flight\" + 0.010*\"travel\" + 0.009*\"driver\" + 0.009*\"drive\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.041*\"modi\" + 0.028*\"gandhi\" + 0.024*\"prime\" + 0.020*\"maharashtra\" + 0.015*\"narendra\" + 0.014*\"defenc\" + 0.014*\"deal\" + 0.014*\"rafal\" + 0.013*\"mumbai\" + 0.013*\"bengal\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.015*\"victim\" + 0.011*\"woman\" + 0.010*\"hospit\" + 0.010*\"girl\" + 0.009*\"spot\" + 0.009*\"death\" + 0.008*\"murder\" + 0.008*\"kill\" + 0.008*\"villag\" + 0.008*\"identifi\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.038*\"reddi\" + 0.023*\"telangana\" + 0.020*\"naidu\" + 0.014*\"andhra\" + 0.011*\"promis\" + 0.008*\"ysrcp\" + 0.007*\"chandrababu\" + 0.006*\"jagan\" + 0.006*\"scheme\" + 0.006*\"hyderabad\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.024*\"protest\" + 0.019*\"attack\" + 0.016*\"kashmir\" + 0.014*\"kill\" + 0.013*\"armi\" + 0.013*\"pakistan\" + 0.013*\"strike\" + 0.011*\"terror\" + 0.010*\"jammu\" + 0.009*\"personnel\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.023*\"bank\" + 0.016*\"compani\" + 0.013*\"scheme\" + 0.012*\"fund\" + 0.009*\"propos\" + 0.009*\"budget\" + 0.009*\"financi\" + 0.008*\"loan\" + 0.008*\"financ\" + 0.008*\"approv\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.041*\"templ\" + 0.032*\"karnataka\" + 0.018*\"sabarimala\" + 0.017*\"mysuru\" + 0.016*\"bengaluru\" + 0.015*\"kumaraswami\" + 0.014*\"gowda\" + 0.011*\"women\" + 0.011*\"devot\" + 0.010*\"pilgrim\"\n",
      "\n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.019*\"social\" + 0.017*\"communiti\" + 0.016*\"media\" + 0.012*\"women\" + 0.011*\"assam\" + 0.009*\"cast\" + 0.007*\"activist\" + 0.007*\"citizen\" + 0.006*\"muslim\" + 0.006*\"societi\"\n",
      "\n",
      "\n",
      "Topic: 11 \n",
      "Words: 0.027*\"bench\" + 0.024*\"petit\" + 0.022*\"suprem\" + 0.018*\"hear\" + 0.014*\"judg\" + 0.010*\"commiss\" + 0.010*\"constitut\" + 0.010*\"petition\" + 0.009*\"appoint\" + 0.009*\"advoc\"\n",
      "\n",
      "\n",
      "Topic: 12 \n",
      "Words: 0.044*\"forest\" + 0.017*\"villag\" + 0.016*\"anim\" + 0.015*\"tree\" + 0.011*\"eleph\" + 0.010*\"mine\" + 0.010*\"conserv\" + 0.009*\"wildlif\" + 0.008*\"tribal\" + 0.008*\"lake\"\n",
      "\n",
      "\n",
      "Topic: 13 \n",
      "Words: 0.025*\"complaint\" + 0.012*\"probe\" + 0.010*\"crimin\" + 0.009*\"sexual\" + 0.009*\"bail\" + 0.009*\"trial\" + 0.009*\"complain\" + 0.009*\"offenc\" + 0.009*\"convict\" + 0.008*\"agenc\"\n",
      "\n",
      "\n",
      "Topic: 14 \n",
      "Words: 0.017*\"film\" + 0.013*\"event\" + 0.011*\"festiv\" + 0.009*\"award\" + 0.008*\"celebr\" + 0.007*\"actor\" + 0.007*\"cultur\" + 0.006*\"world\" + 0.006*\"particip\" + 0.006*\"boat\"\n",
      "\n",
      "\n",
      "Topic: 15 \n",
      "Words: 0.014*\"growth\" + 0.014*\"market\" + 0.011*\"increas\" + 0.011*\"invest\" + 0.010*\"sector\" + 0.009*\"trade\" + 0.009*\"busi\" + 0.008*\"global\" + 0.008*\"industri\" + 0.007*\"million\"\n",
      "\n",
      "\n",
      "Topic: 16 \n",
      "Words: 0.058*\"school\" + 0.029*\"women\" + 0.027*\"educ\" + 0.025*\"children\" + 0.024*\"student\" + 0.017*\"class\" + 0.015*\"teacher\" + 0.011*\"child\" + 0.010*\"parent\" + 0.010*\"train\"\n",
      "\n",
      "\n",
      "Topic: 17 \n",
      "Words: 0.019*\"singh\" + 0.012*\"opposit\" + 0.010*\"modi\" + 0.009*\"prime\" + 0.008*\"statement\" + 0.007*\"question\" + 0.007*\"governor\" + 0.007*\"kejriw\" + 0.006*\"sabha\" + 0.005*\"gandhi\"\n",
      "\n",
      "\n",
      "Topic: 18 \n",
      "Words: 0.021*\"railway\" + 0.019*\"build\" + 0.019*\"construct\" + 0.016*\"train\" + 0.016*\"land\" + 0.013*\"corpor\" + 0.012*\"metro\" + 0.011*\"municip\" + 0.010*\"line\" + 0.008*\"park\"\n",
      "\n",
      "\n",
      "Topic: 19 \n",
      "Words: 0.063*\"kerala\" + 0.023*\"river\" + 0.021*\"panchayat\" + 0.015*\"vijayan\" + 0.010*\"clean\" + 0.010*\"pampa\" + 0.009*\"thoma\" + 0.009*\"inaugur\" + 0.009*\"mission\" + 0.009*\"toilet\"\n",
      "\n",
      "\n",
      "Topic: 20 \n",
      "Words: 0.025*\"flood\" + 0.024*\"worker\" + 0.018*\"relief\" + 0.015*\"damag\" + 0.014*\"affect\" + 0.013*\"rescu\" + 0.012*\"rain\" + 0.012*\"labour\" + 0.012*\"disast\" + 0.010*\"kerala\"\n",
      "\n",
      "\n",
      "Topic: 21 \n",
      "Words: 0.048*\"hospit\" + 0.034*\"health\" + 0.028*\"medic\" + 0.021*\"patient\" + 0.020*\"doctor\" + 0.013*\"treatment\" + 0.011*\"diseas\" + 0.010*\"care\" + 0.009*\"drug\" + 0.007*\"death\"\n",
      "\n",
      "\n",
      "Topic: 22 \n",
      "Words: 0.013*\"technolog\" + 0.012*\"wast\" + 0.011*\"industri\" + 0.010*\"pollut\" + 0.009*\"product\" + 0.008*\"plant\" + 0.007*\"qualiti\" + 0.007*\"compani\" + 0.006*\"manufactur\" + 0.006*\"data\"\n",
      "\n",
      "\n",
      "Topic: 23 \n",
      "Words: 0.066*\"student\" + 0.049*\"univers\" + 0.037*\"colleg\" + 0.022*\"institut\" + 0.016*\"educ\" + 0.014*\"examin\" + 0.011*\"scienc\" + 0.011*\"cours\" + 0.010*\"campus\" + 0.009*\"research\"\n",
      "\n",
      "\n",
      "Topic: 24 \n",
      "Words: 0.013*\"crime\" + 0.012*\"money\" + 0.010*\"seiz\" + 0.009*\"cash\" + 0.009*\"card\" + 0.008*\"phone\" + 0.008*\"church\" + 0.008*\"involv\" + 0.007*\"raid\" + 0.007*\"gold\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3214\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for i in processed_docs:\n",
    "    for j in i:\n",
    "        if(\"corrupt\" in j):\n",
    "            count+=1\n",
    "            \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041*\"tamil\" \n",
      " 0.036*\"nadu\" \n",
      " 0.018*\"aiadmk\" \n",
      " 0.013*\"pakistan\" \n",
      " 0.012*\"chennai\" \n",
      " 0.008*\"stalin\" \n",
      " 0.007*\"prime\" \n",
      " 0.007*\"talk\" \n",
      " 0.007*\"palaniswami\" \n",
      " 0.006*\"modi\"\n",
      "0.065*\"farmer\" \n",
      " 0.026*\"price\" \n",
      " 0.022*\"agricultur\" \n",
      " 0.018*\"crop\" \n",
      " 0.017*\"farm\" \n",
      " 0.011*\"land\" \n",
      " 0.010*\"suppli\" \n",
      " 0.009*\"market\" \n",
      " 0.009*\"product\" \n",
      " 0.009*\"cultiv\"\n",
      "0.033*\"seat\" \n",
      " 0.030*\"poll\" \n",
      " 0.030*\"candid\" \n",
      " 0.028*\"constitu\" \n",
      " 0.027*\"sabha\" \n",
      " 0.021*\"vote\" \n",
      " 0.019*\"contest\" \n",
      " 0.018*\"voter\" \n",
      " 0.016*\"allianc\" \n",
      " 0.011*\"elector\"\n",
      "0.030*\"vehicl\" \n",
      " 0.021*\"transport\" \n",
      " 0.020*\"airport\" \n",
      " 0.015*\"traffic\" \n",
      " 0.013*\"passeng\" \n",
      " 0.012*\"bus\" \n",
      " 0.011*\"flight\" \n",
      " 0.010*\"travel\" \n",
      " 0.009*\"driver\" \n",
      " 0.009*\"drive\"\n",
      "0.041*\"modi\" \n",
      " 0.028*\"gandhi\" \n",
      " 0.024*\"prime\" \n",
      " 0.020*\"maharashtra\" \n",
      " 0.015*\"narendra\" \n",
      " 0.014*\"defenc\" \n",
      " 0.014*\"deal\" \n",
      " 0.014*\"rafal\" \n",
      " 0.013*\"mumbai\" \n",
      " 0.013*\"bengal\"\n",
      "0.015*\"victim\" \n",
      " 0.011*\"woman\" \n",
      " 0.010*\"hospit\" \n",
      " 0.010*\"girl\" \n",
      " 0.009*\"spot\" \n",
      " 0.009*\"death\" \n",
      " 0.008*\"murder\" \n",
      " 0.008*\"kill\" \n",
      " 0.008*\"villag\" \n",
      " 0.008*\"identifi\"\n",
      "0.038*\"reddi\" \n",
      " 0.023*\"telangana\" \n",
      " 0.020*\"naidu\" \n",
      " 0.014*\"andhra\" \n",
      " 0.011*\"promis\" \n",
      " 0.008*\"ysrcp\" \n",
      " 0.007*\"chandrababu\" \n",
      " 0.006*\"jagan\" \n",
      " 0.006*\"scheme\" \n",
      " 0.006*\"hyderabad\"\n",
      "0.024*\"protest\" \n",
      " 0.019*\"attack\" \n",
      " 0.016*\"kashmir\" \n",
      " 0.014*\"kill\" \n",
      " 0.013*\"armi\" \n",
      " 0.013*\"pakistan\" \n",
      " 0.013*\"strike\" \n",
      " 0.011*\"terror\" \n",
      " 0.010*\"jammu\" \n",
      " 0.009*\"personnel\"\n",
      "0.023*\"bank\" \n",
      " 0.016*\"compani\" \n",
      " 0.013*\"scheme\" \n",
      " 0.012*\"fund\" \n",
      " 0.009*\"propos\" \n",
      " 0.009*\"budget\" \n",
      " 0.009*\"financi\" \n",
      " 0.008*\"loan\" \n",
      " 0.008*\"financ\" \n",
      " 0.008*\"approv\"\n",
      "0.041*\"templ\" \n",
      " 0.032*\"karnataka\" \n",
      " 0.018*\"sabarimala\" \n",
      " 0.017*\"mysuru\" \n",
      " 0.016*\"bengaluru\" \n",
      " 0.015*\"kumaraswami\" \n",
      " 0.014*\"gowda\" \n",
      " 0.011*\"women\" \n",
      " 0.011*\"devot\" \n",
      " 0.010*\"pilgrim\"\n",
      "0.019*\"social\" \n",
      " 0.017*\"communiti\" \n",
      " 0.016*\"media\" \n",
      " 0.012*\"women\" \n",
      " 0.011*\"assam\" \n",
      " 0.009*\"cast\" \n",
      " 0.007*\"activist\" \n",
      " 0.007*\"citizen\" \n",
      " 0.006*\"muslim\" \n",
      " 0.006*\"societi\"\n",
      "0.027*\"bench\" \n",
      " 0.024*\"petit\" \n",
      " 0.022*\"suprem\" \n",
      " 0.018*\"hear\" \n",
      " 0.014*\"judg\" \n",
      " 0.010*\"commiss\" \n",
      " 0.010*\"constitut\" \n",
      " 0.010*\"petition\" \n",
      " 0.009*\"appoint\" \n",
      " 0.009*\"advoc\"\n",
      "0.044*\"forest\" \n",
      " 0.017*\"villag\" \n",
      " 0.016*\"anim\" \n",
      " 0.015*\"tree\" \n",
      " 0.011*\"eleph\" \n",
      " 0.010*\"mine\" \n",
      " 0.010*\"conserv\" \n",
      " 0.009*\"wildlif\" \n",
      " 0.008*\"tribal\" \n",
      " 0.008*\"lake\"\n",
      "0.025*\"complaint\" \n",
      " 0.012*\"probe\" \n",
      " 0.010*\"crimin\" \n",
      " 0.009*\"sexual\" \n",
      " 0.009*\"bail\" \n",
      " 0.009*\"trial\" \n",
      " 0.009*\"complain\" \n",
      " 0.009*\"offenc\" \n",
      " 0.009*\"convict\" \n",
      " 0.008*\"agenc\"\n",
      "0.017*\"film\" \n",
      " 0.013*\"event\" \n",
      " 0.011*\"festiv\" \n",
      " 0.009*\"award\" \n",
      " 0.008*\"celebr\" \n",
      " 0.007*\"actor\" \n",
      " 0.007*\"cultur\" \n",
      " 0.006*\"world\" \n",
      " 0.006*\"particip\" \n",
      " 0.006*\"boat\"\n",
      "0.014*\"growth\" \n",
      " 0.014*\"market\" \n",
      " 0.011*\"increas\" \n",
      " 0.011*\"invest\" \n",
      " 0.010*\"sector\" \n",
      " 0.009*\"trade\" \n",
      " 0.009*\"busi\" \n",
      " 0.008*\"global\" \n",
      " 0.008*\"industri\" \n",
      " 0.007*\"million\"\n",
      "0.058*\"school\" \n",
      " 0.029*\"women\" \n",
      " 0.027*\"educ\" \n",
      " 0.025*\"children\" \n",
      " 0.024*\"student\" \n",
      " 0.017*\"class\" \n",
      " 0.015*\"teacher\" \n",
      " 0.011*\"child\" \n",
      " 0.010*\"parent\" \n",
      " 0.010*\"train\"\n",
      "0.019*\"singh\" \n",
      " 0.012*\"opposit\" \n",
      " 0.010*\"modi\" \n",
      " 0.009*\"prime\" \n",
      " 0.008*\"statement\" \n",
      " 0.007*\"question\" \n",
      " 0.007*\"governor\" \n",
      " 0.007*\"kejriw\" \n",
      " 0.006*\"sabha\" \n",
      " 0.005*\"gandhi\"\n",
      "0.021*\"railway\" \n",
      " 0.019*\"build\" \n",
      " 0.019*\"construct\" \n",
      " 0.016*\"train\" \n",
      " 0.016*\"land\" \n",
      " 0.013*\"corpor\" \n",
      " 0.012*\"metro\" \n",
      " 0.011*\"municip\" \n",
      " 0.010*\"line\" \n",
      " 0.008*\"park\"\n",
      "0.063*\"kerala\" \n",
      " 0.023*\"river\" \n",
      " 0.021*\"panchayat\" \n",
      " 0.015*\"vijayan\" \n",
      " 0.010*\"clean\" \n",
      " 0.010*\"pampa\" \n",
      " 0.009*\"thoma\" \n",
      " 0.009*\"inaugur\" \n",
      " 0.009*\"mission\" \n",
      " 0.009*\"toilet\"\n",
      "0.025*\"flood\" \n",
      " 0.024*\"worker\" \n",
      " 0.018*\"relief\" \n",
      " 0.015*\"damag\" \n",
      " 0.014*\"affect\" \n",
      " 0.013*\"rescu\" \n",
      " 0.012*\"rain\" \n",
      " 0.012*\"labour\" \n",
      " 0.012*\"disast\" \n",
      " 0.010*\"kerala\"\n",
      "0.048*\"hospit\" \n",
      " 0.034*\"health\" \n",
      " 0.028*\"medic\" \n",
      " 0.021*\"patient\" \n",
      " 0.020*\"doctor\" \n",
      " 0.013*\"treatment\" \n",
      " 0.011*\"diseas\" \n",
      " 0.010*\"care\" \n",
      " 0.009*\"drug\" \n",
      " 0.007*\"death\"\n",
      "0.013*\"technolog\" \n",
      " 0.012*\"wast\" \n",
      " 0.011*\"industri\" \n",
      " 0.010*\"pollut\" \n",
      " 0.009*\"product\" \n",
      " 0.008*\"plant\" \n",
      " 0.007*\"qualiti\" \n",
      " 0.007*\"compani\" \n",
      " 0.006*\"manufactur\" \n",
      " 0.006*\"data\"\n",
      "0.066*\"student\" \n",
      " 0.049*\"univers\" \n",
      " 0.037*\"colleg\" \n",
      " 0.022*\"institut\" \n",
      " 0.016*\"educ\" \n",
      " 0.014*\"examin\" \n",
      " 0.011*\"scienc\" \n",
      " 0.011*\"cours\" \n",
      " 0.010*\"campus\" \n",
      " 0.009*\"research\"\n",
      "0.013*\"crime\" \n",
      " 0.012*\"money\" \n",
      " 0.010*\"seiz\" \n",
      " 0.009*\"cash\" \n",
      " 0.009*\"card\" \n",
      " 0.008*\"phone\" \n",
      " 0.008*\"church\" \n",
      " 0.008*\"involv\" \n",
      " 0.007*\"raid\" \n",
      " 0.007*\"gold\"\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for ids,topics in lda_model.print_topics(-1):\n",
    "    a = (topics.split(\"+\"))\n",
    "    for j in a:\n",
    "        s = j.split(\"*\")\n",
    "        if \"student\" in a:\n",
    "            print(\"yya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
